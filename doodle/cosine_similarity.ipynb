{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a396a258-e69e-4672-8c25-8cc01e71a4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d7ed2f2-d106-43b1-ad1f-0d40bd2370c1",
   "metadata": {
    "tags": []
   },
   "source": [
    "# *1. Jaccad 유사도*  \n",
    "![Jaccad 유사도](./images/jaccad_sim.png)  \n",
    "\n",
    "# *2. Cosine 유사도*\n",
    "![Cosine 유사도](./images/jaccad_sim.png)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "7df38814-dd73-4bb0-bf83-f0dd33c557c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 단어 전처리해주는 함수 | 정규 표현식으로 알파벳과 숫자만 남기도록 함\n",
    "word_preprocessing = lambda word: re.sub('[^a-zA-Z0-9]', '', word)\n",
    "\n",
    "class similarity:\n",
    "\n",
    "    def __init__(self, word1, word2):\n",
    "        self.word1 = word1\n",
    "        self.word2 = word2\n",
    "\n",
    "    ## 단어 전처리해주는 함수 | 정규 표현식으로 알파벳과 숫자만 남기도록 함\n",
    "    def preprocessing(self, word): \n",
    "        return re.sub('[^a-zA-Z0-9]', '', word)\n",
    "\n",
    "    ## 단어를 벡터화 시키는 함수\n",
    "    def word_vectorizer(self, word):\n",
    "        word = self.preprocessing(word)\n",
    "        word_count = Counter(word.lower())\n",
    "\n",
    "        ## ascii | 0 ~ 9, a ~ z 까지의 문자열을 리스트에 저장 (ascii 코드표에 대응시킴)\n",
    "        ascii = [chr(idx) for idx in range(48, 58)] + [chr(idx) for idx in range(97, 127)]\n",
    "        alphabets = {alpha: 0 for alpha in ascii}\n",
    "\n",
    "        ## 단어에 있는 알파벳 갯수만을 반환시켜줌.\n",
    "        for k, v in word_count.items(): alphabets[k] = v\n",
    "        return np.array([num for num in alphabets.values()])\n",
    "\n",
    "    ## 텍스트를 벡터화 시키는 함수\n",
    "    def text_vectorizer(self):\n",
    "        \n",
    "        ## 공백을 기준으로 텍스트를 나눔.\n",
    "        word1, word2 = self.word1.split(' '), self.word2.split(' ')\n",
    "        \n",
    "        ## 정규표현식을 통해 영어, 숫자만 남기도록 전처리.\n",
    "        word1 = [self.preprocessing(word) for word in word1]\n",
    "        word2 = [self.preprocessing(word) for word in word2]\n",
    "        \n",
    "        ## 정규표현식을 통해 삭제되어 생성된 공백문자 제거\n",
    "        ## e.g)                   words = ['abcd', '무야호!', 'defg']\n",
    "        ##      (정규표현식 거침) words = ['abcd', '', 'defg']\n",
    "        ##            (공백 제거) words = ['abcd', 'defg']\n",
    "        \n",
    "        words1 = [word for word in word1 if word not in '']\n",
    "        words2 = [word for word in word2 if word not in '']\n",
    "\n",
    "        ## 첫 번째 텍스트와 두 번째 텍스트에 있는 단어들로 딕셔너리 생성\n",
    "        words1_dict, words2_dict = {word: 0 for word in words1}, {word: 0 for word in words2}\n",
    "        \n",
    "        ## {텍스트 : 개수}로 구성된 딕셔너리\n",
    "        words1, words2 = Counter(words1), Counter(words2)\n",
    "        \n",
    "        ## words1_dict와 words2_dict를 합쳐 하나의 딕셔너리로 생성\n",
    "        vector1, vector2 = {**words1_dict, **words2_dict},  {**words1_dict, **words2_dict}\n",
    "        \n",
    "        ## vector 딕셔너리에 텍스트에 있는 단어 개수 입력하는 부분\n",
    "        for word, num in words1.items(): vector1[word] = num\n",
    "        for word, num in words2.items(): vector2[word] = num\n",
    "        \n",
    "        ## 텍스트 벡터화 시켜주는 부분\n",
    "        vector1 = np.array([num for num in vector1.values()])\n",
    "        vector2 = np.array([num for num in vector2.values()])\n",
    "\n",
    "        return vector1, vector2\n",
    "\n",
    "    ## Jaccad 유사도 구하는 함수\n",
    "    def jaccad_sim(self):\n",
    "\n",
    "        word1 = self.preprocessing(self.word1)\n",
    "        word2 = self.preprocessing(self.word2)\n",
    "\n",
    "        intersection = set(word1).intersection(set(word2))\n",
    "        union        = set(word1).union(set(word2))\n",
    "\n",
    "        return len(intersection) / len(union)\n",
    "\n",
    "\n",
    "    ## 코사인 유사도 구하는 함수\n",
    "    def cosine_sim(self, task = 'word'):\n",
    "\n",
    "        if 'word' in task.lower():\n",
    "          ## 입력받은 단어 두 개를 벡터화 시켜줌.\n",
    "          vector1 = self.word_vectorizer(self.word1)\n",
    "          vector2 = self.word_vectorizer(self.word2)\n",
    "\n",
    "        elif 'text' in task.lower():\n",
    "            vector1, vector2 = self.text_vectorizer()\n",
    "\n",
    "        ## 코사인 유사도 구해주는 부분\n",
    "        try: cosine = np.dot(vector1, vector2) / (np.linalg.norm(vector1)*np.linalg.norm(vector2))\n",
    "        except: cosine = 0.0\n",
    "\n",
    "        return cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "e76ea012-1c58-4082-b13a-26964e904a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "sim = similarity('calcif', 'calcification')\n",
    "sim2 = similarity('calclifcation', 'calcification')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "defde6cb-e41e-40f2-9249-9821476d1be3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jaccad1 : 0.625, cosine1 : 0.884537962671703\n",
      "jaccad2 : 1.0, cosine2 : 0.9622504486493763\n"
     ]
    }
   ],
   "source": [
    "jaccad1 = sim.jaccad_sim()\n",
    "jaccad2 = sim2.jaccad_sim()\n",
    "\n",
    "cosine1 = sim.cosine_sim()\n",
    "cosine2 = sim2.cosine_sim()\n",
    "\n",
    "print(f'jaccad1 : {jaccad1}, cosine1 : {cosine1}')\n",
    "print(f'jaccad2 : {jaccad2}, cosine2 : {cosine2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "68f15096-bc86-4f98-8645-deea22e41cf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([1, 1, 0]), array([0, 1, 1]))\n",
      "0.4999999999999999\n"
     ]
    }
   ],
   "source": [
    "sim = similarity('micro calcification', 'macro calcification')\n",
    "\n",
    "print(sim.text_vectorizer())\n",
    "print(sim.cosine_sim(task='text'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "27ba3d62-12dd-4282-9f5c-8eedac04bd68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.33609693],\n",
       "       [0.33609693, 1.        ]])"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(['micro calcification', 'macro calcification'])\n",
    "\n",
    "cosine_similarity(tfidf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee711bdf-993d-48f1-a52d-948a7cb507b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dove",
   "language": "python",
   "name": "dove"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
