{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c850603-dbc2-4d08-804d-8f0748c019cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/torch_1.x/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: libtorch_cuda_cu.so: cannot open shared object file: No such file or directory\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from torchvision.utils import save_image\n",
    "from easydict import EasyDict as edict\n",
    "from imutils.paths import list_images\n",
    "from matplotlib import pyplot as plt\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from misc.utils import AdaIN, coral\n",
    "from net import model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "580595d9-ffc3-4ac4-a138-c9dd2b2cae7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = edict({})\n",
    "args.size      = 256\n",
    "args.crop      = True\n",
    "args.style     = 'assets/styles'\n",
    "args.alpha     = 1.0\n",
    "args.device    = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "args.content   = 'assets/contents'\n",
    "args.encoder   = 'assets/vgg_normalised.pth'\n",
    "args.decoder   = 'assets/results/decoder_160000.pth'\n",
    "args.save_ext  = '.jpg'\n",
    "args.save_path = 'assets/output'\n",
    "args.preserve_color        = True\n",
    "args.do_interpolation      = False\n",
    "args.interpolation_weights = ''\n",
    "\n",
    "os.makedirs(args.save_path, exist_ok = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b24e0c94-56cf-48bb-bfa5-32545c154726",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_transform(size, crop):\n",
    "\n",
    "    transform_list = []\n",
    "    if size != 0: transform_list.append(transforms.Resize(size))\n",
    "\n",
    "    if crop: transform_list.append(transforms.CenterCrop(size))\n",
    "\n",
    "    transform_list.append(transforms.ToTensor())\n",
    "\n",
    "    return transforms.Compose(transform_list)\n",
    "\n",
    "\n",
    "def style_transfer(encoder, decoder, content, style, \n",
    "                   alpha = 1.0, interpolation_weights = None):\n",
    "\n",
    "    content_f = encoder(content)\n",
    "    style_f   =   encoder(style)\n",
    "\n",
    "    if interpolation_weights:\n",
    "\n",
    "        _, C, H, W = content_f.size()\n",
    "        feat       = torch.FloatTensor(1, C, H, W).zero_().to(args.device)\n",
    "        base_feat  = AdaIN(content_f, style_f)\n",
    "\n",
    "        for idx, weight in enumerate(interpolation_weights):\n",
    "            feat = feat + weight * base_feat[idx : idx + 1]\n",
    "        content_f = content_f[0:1]\n",
    "        \n",
    "    else: \n",
    "        feat = AdaIN(content_f,style_f)\n",
    "\n",
    "    feat = feat * alpha + content_f * (1 - alpha)\n",
    "    return decoder(feat)\n",
    "                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4fa7309-8c45-43d6-9957-aa3ec805eafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "contents_paths = sorted(list_images(args.content))\n",
    "style_paths    = sorted(list_images(args.style))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a36a2df-2052-40eb-b24b-1ac768406b77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모델 로딩 완.\n"
     ]
    }
   ],
   "source": [
    "decoder = model.Decoder()\n",
    "encoder = model.Encoder()\n",
    "\n",
    "decoder.eval()\n",
    "encoder.eval()\n",
    "\n",
    "decoder.load_state_dict(torch.load(args.decoder))\n",
    "encoder.load_state_dict(torch.load(args.encoder), strict = False)\n",
    "encoder = nn.Sequential(*list(list(encoder.children())[0].children())[:31])\n",
    "\n",
    "decoder.to(args.device)\n",
    "encoder.to(args.device)\n",
    "\n",
    "print('모델 로딩 완.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8acaff2c-5c3a-4bd9-90ba-9b0e17b8b8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "content_tf    = test_transform(args.size, args.crop)\n",
    "style_tf      = test_transform(args.size, args.crop)\n",
    "\n",
    "content_image = Image.open(contents_paths[0]).convert('RGB')\n",
    "style_image   =    Image.open(style_paths[0]).convert('RGB')\n",
    "    \n",
    "content_image = content_tf(content_image)\n",
    "style_image   =     style_tf(style_image)\n",
    "\n",
    "if args.preserve_color:\n",
    "    style_image = coral(style_image, content_image)\n",
    "\n",
    "content_image = content_image.to(args.device).unsqueeze(0)\n",
    "style_image   =   style_image.to(args.device).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c09ee79-5d39-4519-82a3-90a24ac5bf14",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(-0.5, 255.5, 255.5, -0.5)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAGFCAYAAAASI+9IAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAE4ElEQVR4nO3VMQHAMAzAsKz8OWefKbSHhMCfv93dAYCZObcDAHiHKQAQUwAgpgBATAGAmAIAMQUAYgoAxBQAiCkAEFMAIKYAQEwBgJgCADEFAGIKAMQUAIgpABBTACCmAEBMAYCYAgAxBQBiCgDEFACIKQAQUwAgpgBATAGAmAIAMQUAYgoAxBQAiCkAEFMAIKYAQEwBgJgCADEFAGIKAMQUAIgpABBTACCmAEBMAYCYAgAxBQBiCgDEFACIKQAQUwAgpgBATAGAmAIAMQUAYgoAxBQAiCkAEFMAIKYAQEwBgJgCADEFAGIKAMQUAIgpABBTACCmAEBMAYCYAgAxBQBiCgDEFACIKQAQUwAgpgBATAGAmAIAMQUAYgoAxBQAiCkAEFMAIKYAQEwBgJgCADEFAGIKAMQUAIgpABBTACCmAEBMAYCYAgAxBQBiCgDEFACIKQAQUwAgpgBATAGAmAIAMQUAYgoAxBQAiCkAEFMAIKYAQEwBgJgCADEFAGIKAMQUAIgpABBTACCmAEBMAYCYAgAxBQBiCgDEFACIKQAQUwAgpgBATAGAmAIAMQUAYgoAxBQAiCkAEFMAIKYAQEwBgJgCADEFAGIKAMQUAIgpABBTACCmAEBMAYCYAgAxBQBiCgDEFACIKQAQUwAgpgBATAGAmAIAMQUAYgoAxBQAiCkAEFMAIKYAQEwBgJgCADEFAGIKAMQUAIgpABBTACCmAEBMAYCYAgAxBQBiCgDEFACIKQAQUwAgpgBATAGAmAIAMQUAYgoAxBQAiCkAEFMAIKYAQEwBgJgCADEFAGIKAMQUAIgpABBTACCmAEBMAYCYAgAxBQBiCgDEFACIKQAQUwAgpgBATAGAmAIAMQUAYgoAxBQAiCkAEFMAIKYAQEwBgJgCADEFAGIKAMQUAIgpABBTACCmAEBMAYCYAgAxBQBiCgDEFACIKQAQUwAgpgBATAGAmAIAMQUAYgoAxBQAiCkAEFMAIKYAQEwBgJgCADEFAGIKAMQUAIgpABBTACCmAEBMAYCYAgAxBQBiCgDEFACIKQAQUwAgpgBATAGAmAIAMQUAYgoAxBQAiCkAEFMAIKYAQEwBgJgCADEFAGIKAMQUAIgpABBTACCmAEBMAYCYAgAxBQBiCgDEFACIKQAQUwAgpgBATAGAmAIAMQUAYgoAxBQAiCkAEFMAIKYAQEwBgJgCADEFAGIKAMQUAIgpABBTACCmAEBMAYCYAgAxBQBiCgDEFACIKQAQUwAgpgBATAGAmAIAMQUAYgoAxBQAiCkAEFMAIKYAQEwBgJgCADEFAGIKAMQUAIgpABBTACCmAEBMAYCYAgAxBQBiCgDEFACIKQAQUwAgpgBATAGAmAIAMQUAYgoAxBQAiCkAEFMAIKYAQEwBgJgCADEFAGIKAMQUAIgpABBTACCmAEBMAYCYAgAxBQBiCgDEFACIKQAQUwAgpgBATAGAmAIAMQUAYgoAxBQAiCkAEFMAIKYAQEwBgJgCADEFAGIKAMQUAIgpABBTACCmAEBMAYCYAgAxBQBiCgDEFACIKQAQUwAgpgBATAGAmAIAMQUAYgoAxBQAiCkAEFMAIKYAQEwBgJgCADEFAGIKAMQUAIgpABBTACCmAEBMAYCYAgAxBQBiCgDEFACIKQAQUwAgpgBAfu8DBwYENNNsAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    output = style_transfer(encoder, decoder, content_image, style_image, args.alpha)\n",
    "\n",
    "output = output.cpu().numpy()\n",
    "output = np.transpose(output.squeeze(), (1, 2, 0))\n",
    "\n",
    "plt.imshow(output * 255)\n",
    "plt.axis(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4e546b-2ba6-430d-8983-b4cfed9c6c63",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_1.x",
   "language": "python",
   "name": "torch_1.x"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
