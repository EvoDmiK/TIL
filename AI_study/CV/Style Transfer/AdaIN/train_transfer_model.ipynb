{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3fb7f785-e9f2-4cd4-a672-346d18f6f0dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/torch_1.x/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/root/anaconda3/envs/torch_1.x/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: libtorch_cuda_cu.so: cannot open shared object file: No such file or directory\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from easydict import EasyDict as edict\n",
    "from tensorboardX import SummaryWriter\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "from PIL import Image\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from dataset import dataset\n",
    "from dataset import sampler\n",
    "from net.model import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8540e29-6bc0-46cb-90ef-1c1fd6f60b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "args                    = edict({})\n",
    "args.lr                 = 1e-4\n",
    "args.device             = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "args.epochs             = 160_000\n",
    "args.lr_decay           = 5e-5\n",
    "args.save_dir           = 'assets/results'\n",
    "args.n_threads          = 16\n",
    "args.batch_size         = 8 \n",
    "args.style_path         = 'assets/styles'\n",
    "args.style_weight       = 10.0\n",
    "\n",
    "## reference : https://drive.google.com/file/d/1EpkBA2K2eYILDSyPTt0fztz59UjAIpZU/view\n",
    "args.encoder_path       = 'misc/vgg_normalised.pth'\n",
    "args.contents_path      = 'assets/contents'\n",
    "args.content_weight     = 1.0\n",
    "args.save_ckpt_interval = 10_000\n",
    "\n",
    "os.makedirs(          args.save_dir, exist_ok = True)\n",
    "os.makedirs(f'{args.save_dir}/logs', exist_ok = True)\n",
    "writer = SummaryWriter(log_dir = f'{args.save_dir}/logs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d66b11e7-c8b0-406d-a8c6-4f18af5a2df6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모델 구성 완.\n"
     ]
    }
   ],
   "source": [
    "decoder = Decoder()\n",
    "encoder = Encoder()\n",
    "encoder.load_state_dict(torch.load(args.encoder_path), strict = False)\n",
    "\n",
    "encoder = nn.Sequential(*list(encoder.children())[:31])\n",
    "model   = Net(encoder, decoder).to(args.device)\n",
    "model.train()\n",
    "\n",
    "print('모델 구성 완.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0844c388-5f22-4083-ac66-3dbd4482695e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_lr(optimizer, iter_cnt):\n",
    "\n",
    "    lr = args.lr / (1.0 + args.lr_decay * iter_cnt)\n",
    "    for param in optimizer.param_groups: param['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b869b47-abc9-4f2b-ac6f-61b7f2de97ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "content_tf = dataset.train_transform()\n",
    "style_tf   = dataset.train_transform()\n",
    "\n",
    "content_dataset = dataset.StyleTransferDataset(args.contents_path, content_tf)\n",
    "style_dataset   = dataset.StyleTransferDataset(   args.style_path,   style_tf)\n",
    "\n",
    "content_iter    = iter(DataLoader(content_dataset, batch_size = args.batch_size,\n",
    "                                  sampler = sampler.InfiniteSamplerWrapper(content_dataset),\n",
    "                                  num_workers = args.n_threads))\n",
    "\n",
    "style_iter      = iter(DataLoader(style_dataset, batch_size = args.batch_size,\n",
    "                                  sampler = sampler.InfiniteSamplerWrapper(style_dataset),\n",
    "                                  num_workers = args.n_threads))\n",
    "\n",
    "optimizer       = torch.optim.Adam(model.decoder.parameters(), lr = args.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a4a509cf-5803-460c-9d51-7d3cd4e7cb3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 3295/160000 [03:28<2:44:53, 15.84it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     14\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m---> 16\u001b[0m writer\u001b[38;5;241m.\u001b[39madd_scalar(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent loss\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[43mloss_c\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, epoch)\n\u001b[1;32m     17\u001b[0m writer\u001b[38;5;241m.\u001b[39madd_scalar(  \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstyle loss\u001b[39m\u001b[38;5;124m'\u001b[39m, loss_s\u001b[38;5;241m.\u001b[39mitem(), epoch)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m%\u001b[39m args\u001b[38;5;241m.\u001b[39msave_ckpt_interval \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m epoch \u001b[38;5;241m==\u001b[39m args\u001b[38;5;241m.\u001b[39mepochs:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in tqdm(range(1, args.epochs + 1)):\n",
    "\n",
    "    adjust_lr(optimizer, iter_cnt = epoch)\n",
    "    content_images = next(content_iter).to(args.device)\n",
    "    style_images   = next(style_iter).to(args.device)\n",
    "\n",
    "    loss_c, loss_s = model(content_images, style_images)\n",
    "    loss_c         = args.content_weight * loss_c\n",
    "    loss_s         =   args.style_weight * loss_s\n",
    "    loss           = loss_c + loss_s\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    writer.add_scalar('content loss', loss_c.item(), epoch)\n",
    "    writer.add_scalar(  'style loss', loss_s.item(), epoch)\n",
    "\n",
    "    if epoch % args.save_ckpt_interval == 0 or epoch == args.epochs:\n",
    "\n",
    "        print(f'content loss : {loss_c.item():.3f}')\n",
    "        print(f'  style loss : {loss_s.item():.3f}')\n",
    "        print(f'        loss : {loss.item():.3f}')\n",
    "        state_dict = model.decoder.state_dict()\n",
    "        for k in state_dict.keys(): state_dict[k] = state_dict[k].to(torch.device('cpu'))\n",
    "\n",
    "        torch.save(state_dict, f'{args.save_dir}/decoer_{str(epoch).zfill(6)}.pth.tar')\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e868d93-ed83-4240-b133-57a5e6c87b0e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_1.x",
   "language": "python",
   "name": "torch_1.x"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
