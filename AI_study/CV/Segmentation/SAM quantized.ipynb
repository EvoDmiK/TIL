{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72a21326-88af-4dab-a4ac-933c91897640",
   "metadata": {},
   "outputs": [],
   "source": [
    "from onnxruntime.quantization.quantize import quantize_dynamic\n",
    "from segment_anything.utils.onnx import SamOnnxModel\n",
    "from onnxruntime.quantization import QuantType\n",
    "from segment_anything import SamPredictor\n",
    "from segment_anything import build_sam\n",
    "import onnxruntime\n",
    "import numpy as np\n",
    "import torch\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5fefb7b-79ce-411f-9674-9dcd9203b660",
   "metadata": {},
   "outputs": [],
   "source": [
    "model        = build_sam('checkpoint/sam_vit_h.pth')\n",
    "onnx_model   = SamOnnxModel(model = model, return_single_mask = True)\n",
    "dynamic_axes = {\n",
    "                    'point_coords' : {1 : 'num_points'},\n",
    "                    'point_labels' : {1 : 'num_points'}\n",
    "              \n",
    "                }\n",
    "\n",
    "embed_dim  = model.prompt_encoder.embed_dim\n",
    "embed_size = model.prompt_encoder.image_embedding_size\n",
    "\n",
    "mask_input_size = [4 * x for x in embed_size]\n",
    "dummy_inputs    = {\n",
    "                        'image_embeddings' : torch.randn(1, embed_dim, *embed_size, dtype = torch.float),\n",
    "                        'point_coords'     : torch.randint(low = 0, high = 1024, size = (1, 5, 2), dtype = torch.float),\n",
    "                        'point_labels'     : torch.randint(low = 0, high = 4, size = (1, 5), dtype = torch.float),\n",
    "                        'mask_input'       : torch.randn(1, 1, *mask_input_size, dtype = torch.float),\n",
    "                        'has_mask_input'   : torch.tensor([1], dtype=torch.float),\n",
    "                        'orig_im_size'     : torch.tensor([1500, 2250])\n",
    "                    }\n",
    "\n",
    "_ = onnx_model(**dummy_inputs)\n",
    "output_names = ['masks', 'iou_predictions', 'low_res_masks']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c8300187-d1ca-4a96-a169-45d916b2127a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/torch/lib/python3.10/site-packages/segment_anything/modeling/transformer.py:232: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  attn = attn / math.sqrt(c_per_head)\n",
      "/opt/conda/envs/torch/lib/python3.10/site-packages/segment_anything/utils/onnx.py:97: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  score_reweight = torch.tensor(\n",
      "/opt/conda/envs/torch/lib/python3.10/site-packages/torch/onnx/symbolic_opset9.py:5589: UserWarning: Exporting aten::index operator of advanced indexing in opset 16 is achieved by combination of multiple ONNX operators, including Reshape, Transpose, Concat, and Gather. If indices include negative values, the exported graph will produce incorrect results.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============= Diagnostic Run torch.onnx.export version 2.0.0+cu117 =============\n",
      "verbose: False, log level: Level.ERROR\n",
      "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "onnx_model_path  = 'checkpoint/onnx_sam.onnx'\n",
    "quant_model_path = 'checkpoint/quant_sam.onnx' \n",
    "\n",
    "with open('checkpoint/onnx_sam.onnx', 'wb') as f:\n",
    "    \n",
    "    torch.onnx.export(\n",
    "                        onnx_model, tuple(dummy_inputs.values()),\n",
    "                        f, export_params = True, verbose = False, opset_version = 16,\n",
    "                        do_constant_folding = True, input_names = list(dummy_inputs.keys()),\n",
    "                        output_names = output_names, dynamic_axes = dynamic_axes\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "babd5afc-59c2-4c8b-b339-30d5debe7cf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignore MatMul due to non constant B: /[/transformer/layers.0/self_attn/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/transformer/layers.0/self_attn/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/transformer/layers.0/cross_attn_token_to_image/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/transformer/layers.0/cross_attn_token_to_image/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/transformer/layers.0/cross_attn_image_to_token/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/transformer/layers.0/cross_attn_image_to_token/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/transformer/layers.1/self_attn/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/transformer/layers.1/self_attn/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/transformer/layers.1/cross_attn_token_to_image/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/transformer/layers.1/cross_attn_token_to_image/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/transformer/layers.1/cross_attn_image_to_token/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/transformer/layers.1/cross_attn_image_to_token/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/transformer/final_attn_token_to_image/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/transformer/final_attn_token_to_image/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/MatMul_1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'checkpoint/quant_sam.onnx'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quantize_dynamic(\n",
    "    model_input    = onnx_model_path,\n",
    "    model_output   = quant_model_path,\n",
    "    optimize_model = True,\n",
    "    per_channel    = False, \n",
    "    reduce_range   = False,\n",
    "    weight_type    = QuantType.QUInt8\n",
    ")\n",
    "\n",
    "onnx_model_path = quant_model_path\n",
    "onnx_model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97430b21-4bf9-41d7-814e-101b69eda589",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/torch/lib/python3.10/site-packages/torch/cuda/__init__.py:546: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n"
     ]
    }
   ],
   "source": [
    "ort_session = onnxruntime.InferenceSession(onnx_model_path)\n",
    "\n",
    "model.to(device = 'cuda')\n",
    "predictor = SamPredictor(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f0737d29-e815-41bd-ae97-44fe1b5bc11c",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = cv2.cvtColor(cv2.imread('checkpoint/src_image.jpg'), cv2.COLOR_BGR2RGB)\n",
    "predictor.set_image(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec612aba-7bb2-430d-9b0d-4e10fa44f169",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 256, 64, 64)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_embedding = predictor.get_image_embedding().cpu().numpy()\n",
    "image_embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c4482e3-8275-45da-b520-ce5d8031213c",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_point = np.array([[500, 375]])\n",
    "input_label = np.array([1])\n",
    "\n",
    "onnx_coord  = np.concatenate([input_point, np.array([[0.0, 0.0]])], axis = 0)[None, :, :]\n",
    "onnx_label  = np.concatenate([input_label, np.array([-1])], axis = 0)[None, :].astype(np.float32)\n",
    "onnx_coord  = predictor.transform.apply_coords(onnx_coord, image.shape[:2]).astype(np.float32)\n",
    "\n",
    "onnx_mask_input     = np.zeros((1, 1, 256, 256), dtype = np.float32)\n",
    "onnx_has_mask_input = np.zeros(1, dtype = np.float32) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da96104-3c27-4ec7-bf04-a69f57a18a90",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
