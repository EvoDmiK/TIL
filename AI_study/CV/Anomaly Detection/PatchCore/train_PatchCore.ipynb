{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91a12c5b-e45b-4379-b046-a3d6c8ef8ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import shutil\n",
    "import glob\n",
    "import abc\n",
    "import os\n",
    "\n",
    "from sklearn.random_projection import SparseRandomProjection\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.models import wide_resnet50_2\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from scipy.ndimage import gaussian_filter\n",
    "from easydict import EasyDict as edict\n",
    "from torch.nn import functional as F\n",
    "from torchvision import transforms\n",
    "import pytorch_lightning as pl\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import faiss\n",
    "import torch\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48643dbc-692e-4c53-be3a-696a21f0ff60",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_PATH = '/'.join(os.getcwd().split('/')[:-3])\n",
    "DATA_PATH = f'{ROOT_PATH}/dataset/MVTecDataset/pill'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5795be22-45e6-4d3d-9759-3e2679085235",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SamplingMethod(object):\n",
    "    \n",
    "    __metaclass__ = abc.ABCMeta\n",
    "    \n",
    "    @abc.abstractmethod\n",
    "    def __init__(self, x, y, seed, **kwargs):\n",
    "        \n",
    "        self.x    = x\n",
    "        self.y    = y\n",
    "        self.seed = seed\n",
    "        \n",
    "        \n",
    "    def flatten_x(self):\n",
    "        \n",
    "        shape  = self.x.shape\n",
    "        flat_x = self.x\n",
    "        \n",
    "        if len(shape) > 2: flat_x = np.reshape(self.x, (shape[0], np.product(shape[1:])))\n",
    "        \n",
    "        return flat_x\n",
    "    \n",
    "    \n",
    "    @abc.abstractmethod\n",
    "    def _select_batch(self): return\n",
    "\n",
    "\n",
    "    def select_batch(self, **kwargs): return self._select_batch(**kwargs) \n",
    "\n",
    "\n",
    "    def to_dict(self): return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bee9680e-77ea-41d9-832c-41e0700bc1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KCenterGreedy(SamplingMethod):\n",
    "    \n",
    "    def __init__(self, x, y, seed, metric = 'euclidean'):\n",
    "        \n",
    "        self.x                = x\n",
    "        self.y                = y\n",
    "        self.n_obs            = self.x.shape[0]\n",
    "        self.metric           = metric\n",
    "        self.flat_x           = self.flatten_x()\n",
    "        self.features         = self.flat_x\n",
    "        self.min_dist         = None\n",
    "        self.already_selected = []\n",
    "        \n",
    "        \n",
    "    def update_distances(self, cluster_centers, only_new = True, reset_dist = False):\n",
    "        \n",
    "        if reset_dist: self.min_dist = None\n",
    "        if   only_new: cluster_centers = [d for d in cluster_centers\n",
    "                                          if d not in self.already_selected]\n",
    "        \n",
    "        if cluster_centers:\n",
    "            \n",
    "            x    = self.features[cluster_centers]\n",
    "            dist = pairwise_distances(self.features, x, metric = self.metric)\n",
    "            \n",
    "            self.min_dist = np.min(dist, axis = 1).reshape(-1, 1) if self.min_dist is None \\\n",
    "                            else np.minimum(self.min_dist, dist)     \n",
    "            \n",
    "            \n",
    "    def select_batch_(self, model, already_selected, N, **kwargs):\n",
    "        \n",
    "        try:\n",
    "            print('Getting transformed features...')\n",
    "            self.features = model.transform(self.x)\n",
    "            print('Compute distances...')\n",
    "            \n",
    "            self.update_distances(already_selected, only_new = False, reset_dist = True)\n",
    "            \n",
    "        except:\n",
    "            \n",
    "            print('Using flat_X as features')\n",
    "            self.update_distances(already_selected, only_new = True, reset_dist = False)\n",
    "            \n",
    "            \n",
    "        new_batch = []\n",
    "        \n",
    "        for _ in range(N):\n",
    "            \n",
    "            idx = np.random.choice(np.arange(self.n_obs)) if self.already_selected is None \\\n",
    "                  else np.argmax(self.min_distances)\n",
    "            \n",
    "            assert idx not in already_selected\n",
    "            self.update_distances([idx], only_new = True, reset_dist = False)\n",
    "            new_batch.append(idx)\n",
    "            \n",
    "        print(f'Maximum distance from cluster centers is {self.min_dist:.2f}')\n",
    "        self.already_selected = already_selected\n",
    "        \n",
    "        \n",
    "        return new_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "64bd4b71-3b84-475f-af28-de1c6e77256a",
   "metadata": {},
   "outputs": [],
   "source": [
    "makedirs    = lambda path: os.makedirs(path, exist_ok = True)\n",
    "cvt2heatmap = lambda gray: cv2.applyColorMap(np.uint8(gray), cv2.COLORMAP_JET) \n",
    "\n",
    "def distance_matrix(x, y = None, p = 2):\n",
    "    \n",
    "    y    = x if type(y) == type(None) else y\n",
    "    \n",
    "    n    = x.size(0)\n",
    "    m    = y.size(0)\n",
    "    d    = x.size(1)\n",
    "    \n",
    "    x    = x.unsqueeze(1).expand(n, m, d)\n",
    "    y    = y.unsqueeze(0).expand(n, m, d)\n",
    "    \n",
    "    dist = torch.pow(x - y, p).sum(2)\n",
    "    \n",
    "    return dist\n",
    "\n",
    "\n",
    "def copy_files(src, dst, ignores = []):\n",
    "    \n",
    "    file_names = os.listdir(src)\n",
    "    for file_name in file_names:\n",
    "        \n",
    "        ignore_check = [True for idx in ignores if idx in file_name]\n",
    "        if ignore_check: continue\n",
    "        \n",
    "        full_file_name = os.path.join(src, file_name)\n",
    "        if os.path.isfile(full_file_name): \n",
    "            shutil.copy(full_file_name, os.path.join(dst, file_name))\n",
    "            \n",
    "        if os.path.isdir(full_File_name):\n",
    "            os.makedirs(os.path.join(dst, file_name), exist_ok = True)\n",
    "            copy_files(full_file_name, os.path.join(dst, file_name), ignores)\n",
    "            \n",
    "\n",
    "            \n",
    "def prep_dirs(root):\n",
    "    \n",
    "    embeddings_path = os.path.join('./', args.model_path)  \n",
    "    sample_path     = os.path.join(root, 'sample')\n",
    "    source_path     = os.path.join(root, 'src')\n",
    "    \n",
    "    makedirs(embeddings_path)\n",
    "    makedirs(sample_path)\n",
    "    makedirs(source_path)\n",
    "    \n",
    "    return embeddings_path, sample_path, source_path\n",
    "\n",
    "\n",
    "def heatmap_on_image(heatmap, image):\n",
    "    \n",
    "    shape = image.shape\n",
    "    if heatmap.shape != image.shape: heatmap = cv2.resize(heatmap, (shape[0], shape[1]))\n",
    "    \n",
    "    out   = np.float32(heatmap) / 255 + np.float32(image) / 255\n",
    "    out   = out / np.max(out)\n",
    "    \n",
    "    return np.uint8(255 * out)\n",
    "\n",
    "\n",
    "def mix_max_norm(image):\n",
    "    \n",
    "    im_min, im_max = image.min(), image.max()\n",
    "    return im_min, im_max, (image - im_min) / (im_max - im_min)\n",
    "\n",
    "\n",
    "def fixed_min_max_norm(image, min_value, max_value):\n",
    "    \n",
    "    im_min, im_max = image.min(), image.max()\n",
    "    return im_min, im_max, (image - min_value) / (max_value - min_value)\n",
    "    \n",
    "    \n",
    "\n",
    "def cal_confusion_matrix(gt, pred_no_thr, thr, image_path_list):\n",
    "    \n",
    "    pred_thr, false_n, false_p = [], [], []\n",
    "    \n",
    "    for idx, pred in enumerate(pred_no_thr):\n",
    "        \n",
    "        if pred > thr:\n",
    "            \n",
    "            pred_thr.append(1)\n",
    "            if gt[idx] == 0: false_p.append(image_path.list[idx])\n",
    "            \n",
    "        else:\n",
    "            pred_thr.append(0)\n",
    "            if gt[idx] == 1: false_n. append(image_path_list[idx])\n",
    "            \n",
    "            \n",
    "    conf_mat = confusion_matrix(gt, pred_thr)\n",
    "    \n",
    "    print(conf_mat)\n",
    "    print(f'# of false positive : {false_p}')\n",
    "    print(f'# of false negative : {false_n}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e2c0e49d-3b4f-4d0b-bcd0-4794a6053d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN:\n",
    "    \n",
    "    def __init__(self, x = None, y = None, p = 2):\n",
    "        \n",
    "        self.p = p\n",
    "        self.train(x, y)\n",
    "        \n",
    "    \n",
    "    def train(self, x, y):\n",
    "        \n",
    "        self.train_pts = x\n",
    "        self.train_lb  = y\n",
    "        \n",
    "    \n",
    "    def __call__(self, x): \n",
    "        \n",
    "        return self.predict(x)\n",
    "\n",
    "    \n",
    "    def predict(self, x):\n",
    "        \n",
    "        condition = [type(self.train_pts) == type(None),\n",
    "                     type(self.train_lb)  == type(None)]\n",
    "        \n",
    "        if any(condition):\n",
    "            \n",
    "            name == self.__class__.name\n",
    "            raise RuntimeError(f'{name} wasn\\'t trained. \\nNeed to execute {name}.train() first')\n",
    "            \n",
    "        dist   = distance_matrix(x, self.train_pts, self.p) ** (1 / self.p)\n",
    "        labels = torch.argmin(dist, dim = 1)\n",
    "        \n",
    "        return self.train_label[labels]\n",
    "    \n",
    "\n",
    "class KNN(NN):\n",
    "    \n",
    "    def __init__(self, x = None, y = None, k = 3, p = 2):\n",
    "        \n",
    "        super().__init__(x, y, p)\n",
    "        self.k = k\n",
    "        \n",
    "        \n",
    "    def train(self, x, y):\n",
    "        \n",
    "        super().train(x, y)\n",
    "        if type(y) != type(None):\n",
    "            self.unique_labels = self.train_label.unique()\n",
    "            \n",
    "    \n",
    "    def predict(self, x):\n",
    "        \n",
    "        dist = distance_matrix(x, self.train_pts, self.p) ** (1 / self.p)\n",
    "        knn  = dist.topk(self.k, largest = False)\n",
    "        \n",
    "        return knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "67f9d3a9-4228-4475-89b3-06d2eb5fa2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding_concat(x, y):\n",
    "    \n",
    "    B, C1, H1, W1 = x.size()\n",
    "    _, C2, H2, W2 = y.size()\n",
    "    \n",
    "    ratio = int(H1 / H2)\n",
    "    x     = F.unfold(x, kernel_size = ratio, dilation = 1, stride = ratio)\n",
    "    x     = x.view(B, C1, -1, H2, W2)\n",
    "    z     = torch.zeros(B, C1 + C2, x.size(2), H2, W2)\n",
    "    \n",
    "    for idx in range(x.size(2)): \n",
    "        z[:, :, idx, :, :] = torch.cat((x[:, :, idx, :, :], y), 1)\n",
    "        \n",
    "    z = z.view(B, -1, H2 * W2)\n",
    "    z = F.fold(z, kernel_size = ratio, output_size = (H1, W1), stride = ratio)\n",
    "    \n",
    "    return z\n",
    "\n",
    "\n",
    "def reshape_embedding(embedding):\n",
    "    \n",
    "    embedding_list = []\n",
    "    for i in range(embedding.shape[0]):\n",
    "        \n",
    "        for j in range(embedding.shape[2]):\n",
    "            \n",
    "            for k in range(embedding.shape[3]):\n",
    "                \n",
    "                embedding_list.append(embedding[i, :, j, k])\n",
    "                \n",
    "    return embedding_list\n",
    "\n",
    "\n",
    "def get_args():\n",
    "    \n",
    "    config                           = {}\n",
    "    config['coreset_sampling_ratio'] = 0.001\n",
    "    config['save_anomaly_map']       = True\n",
    "    config['dataset_path']           = DATA_PATH\n",
    "    config['output_path']            = 'output'\n",
    "    config['n_neighbors']            = 9\n",
    "    config['batch_size']             = 1\n",
    "    config['input_size']             = 224\n",
    "    config['model_path']             = 'model'\n",
    "    config['load_size']              = 256\n",
    "    config['epochs']                 = 3\n",
    "    config['phase']                  = 'train'\n",
    "    \n",
    "    return edict(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6ea3f98a-cca0-4e9e-bcef-083354e6222d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MVTecDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, root, transform, phase):\n",
    "        \n",
    "        if phase == 'train': self.image_path = os.path.join(root, 'train')\n",
    "        else: self.image_path = os.path.join(root, 'test')\n",
    "        \n",
    "        self.transform   = transform\n",
    "        self.image_paths, self.labels, self.types = self.load_dataset()\n",
    "        \n",
    "        \n",
    "    def load_dataset(self):\n",
    "        \n",
    "        total_paths, total_labels, total_types = [], [], []\n",
    "        defect_types = os.listdir(self.image_path)\n",
    "        \n",
    "        for defect_type in defect_types:\n",
    "            \n",
    "            defect_path = os.path.join(self.image_path, defect_type)\n",
    "            image_paths = f'{glob.glob(defect_path)}/*.*'\n",
    "            \n",
    "            if defect_type == 'good': \n",
    "                total_paths.extend(image_paths)\n",
    "                total_labels.extend([0] * len(image_paths))\n",
    "                total_types.extend(['good'] * len(image_paths))\n",
    "                \n",
    "            else:\n",
    "                image_paths.sort()\n",
    "                total_paths.extend([1] * len(image_paths))\n",
    "                total_types.extend([defect_type] * len(image_paths))\n",
    "                \n",
    "        return total_paths, total_labels, total_types\n",
    "    \n",
    "    \n",
    "    def __len__(self): return len(self.image_paths)\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        image_path = self.image_paths[idx]\n",
    "        label      = self.labels[idx]\n",
    "        type_      = self.types[idx]\n",
    "        \n",
    "        image      = Image.open(image_path).convert('RGB')\n",
    "        image      = self.transform(image)\n",
    "        \n",
    "        return image, label, os.path.basename(image_path[:-4]), type_                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f2dbca26-0ff1-4161-86a5-3aa437791f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "class STPM(pl.LightningModule):\n",
    "    \n",
    "    def __init__(self, hparams):\n",
    "        \n",
    "        super(STPM, self).__init__()\n",
    "        # self.save_hparams(hparams)\n",
    "        self.init_features()\n",
    "        \n",
    "        def hook_t(module, input, output): self.features.append(output)\n",
    "        \n",
    "        self.model = wide_resnet50_2(pretrained = True)\n",
    "        for param in self.model.parameters(): param.requires_grad = True\n",
    "        \n",
    "        self.model.layer2[-1].register_forward_hook(hook_t)\n",
    "        self.model.layer3[-1].register_forward_hook(hook_t)\n",
    "        \n",
    "        self.criterion = torch.nn.MSELoss(reduction = 'sum')\n",
    "        self.init_results_list()\n",
    "        \n",
    "        self.transforms = transforms.Compose([\n",
    "                                transforms.Resize((args.load_size, args.load_size), Image.ANTIALIAS),\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.CenterCrop(args.input_size)\n",
    "                            ])\n",
    "        \n",
    "        \n",
    "    def init_results_list(self):\n",
    "        \n",
    "        self.image_path   = []\n",
    "        self.px_lvl_pred  = []\n",
    "        self.img_lvl_pred = []\n",
    "        \n",
    "    \n",
    "    def init_features(self): self.features = []\n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        self.init_features()\n",
    "        _ = self.model(x)\n",
    "        return self.features\n",
    "\n",
    "    \n",
    "    def save_anomaly_map(self, anomaly_map, input_image, file_name, type_):\n",
    "        \n",
    "        if anomaly_map.shape != input_image.shape:\n",
    "            shape        = input_image.shape\n",
    "            anomaly_map  = cv2.resize(anomaly_map, (shape[0], shape[1]))\n",
    "            \n",
    "        anomaly_map_norm = min_max_norm(anomaly_map)\n",
    "        anomaly_map_hm   = cvt2heatmap(anomaly_map_norm * 255)\n",
    "        \n",
    "        (H, W) = input_image.shape[:2]\n",
    "        cv2.imwrite(f'{self.sample_path}/{type_}_{file_name}.jpg', input_image)\n",
    "        cv2.imwrite(f'{self.sample_path}/{type_}_{file_name}_hm.jpg', anomaly_map_hm)\n",
    "        \n",
    "        \n",
    "    def fixed_save_anomaly_map(self, anomaly_map, input_image, values, names):\n",
    "        \n",
    "        makedirs('image_logs')\n",
    "        \n",
    "        file_name, folder_name           = names\n",
    "        min_val, max_val, (lower, upper) = values\n",
    "        \n",
    "        lower     = np.array(lower, dtype = np.uint8)\n",
    "        upper     = np.array(upper, dtype = np.uint8)\n",
    "        \n",
    "        if anomaly_map.shape != input_image.shape:\n",
    "            shape        = input_image.shape\n",
    "            anomaly_map  = cv2.resize(anomaly_map, (shape[0], shape[1]))\n",
    "            \n",
    "        im_min, im_max, anomaly_map_norm = fixed_min_max_norm(anomaly_map, min_val, max_val)\n",
    "        print(f'min value : {im_min:.2f}, max value : {im_max:.2f}')\n",
    "        \n",
    "        anomaly_map_hm = cvt2heatmap(anomaly_map_norm * 255)\n",
    "        hm_on_image    = heatmap_on_image(anomaly_map_hm, input_image)\n",
    "        \n",
    "        anomaly_hsv    = cv2.cvtColor(anomaly_map_hm, cv2.COLOR_BGR2HSV)\n",
    "        mask           = cv2.inRange(anomaly_hsv, lower, upper)\n",
    "        output         = cv2.bitwise_and(anomaly_map_hm, anomaly_map_hm, mask = mask)\n",
    "        \n",
    "        _, thr         = cv2.threshold(mask, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "        draw_image     = input_image.copy()\n",
    "        conts, _       = cv2.findContours(thr, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        \n",
    "        label          = 'anomaly' if len(conts) != 0 else 'good'\n",
    "        for cont in conts: cv2.drawContours(draw_image, [cont], 0, (0, 0, 255), 2)\n",
    "        \n",
    "        save_path = f'{self.sample_path}/{label}'\n",
    "        makedirs(save_path)\n",
    "        cv2.imwrite(os.path.join(save_path, f'{file_name}.jpg'), input_image)\n",
    "        cv2.imwrite(os.path.join(save_path, f'{file_name}_mask.jpg'), output)\n",
    "        cv2.imwrite(os.path.join(save_path, f'{file_name}_draw.jpg'), draw_image)\n",
    "        cv2.imwrite(os.path.join(save_path, f'{file_name}_hm.jpg'), anomaly_map_hm)\n",
    "        cv2.imwrite(os.path.join(save_path, f'{file_name}_hm_on_img.jpg'), hm_on_image)\n",
    "        \n",
    "        text = f'image path : {folder_name}/{file_name}\\nimage pixel minimum : {image_min:.2f}\\nimage pixel maximum : {image_max:.2f}\\n\\n\\n'\n",
    "        print(text)\n",
    "        \n",
    "        \n",
    "    def train_dataloader(self):\n",
    "        \n",
    "        train_datasets = MVTecDataset(root = args.dataset_path, transforms = self.transforms, phase = 'train')\n",
    "        train_loader   = DataLoader(image_datasets, batch_size = args.batch_size, shuffle = True)\n",
    "        return train_loader\n",
    "    \n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        \n",
    "        test_datasets = MVTecDataset(root = args.dataset_path, transforms = self.transforms, phase = 'test')\n",
    "        test_loader   = DataLoader(image_datasets, batch_size = 1, shuffle = False)\n",
    "        return test_loader\n",
    "    \n",
    "    \n",
    "    def configure_optimizers(self): return None\n",
    "\n",
    "    \n",
    "    def on_train_start(self):\n",
    "        \n",
    "        self.model.train()\n",
    "        self.embedding_dir, self.sample_path, self.code_save_path = prep_dirs(self.log_dir)\n",
    "        self.embedding_list = []\n",
    "        \n",
    "    \n",
    "    def on_test_start(self):\n",
    "        \n",
    "        self.init_results_list()\n",
    "        self.embedding_dir, self.sample_path, self.code_save_path = prep_dirs(self.log_dir)\n",
    "        \n",
    "        \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        \n",
    "        x, _, file_name, _ = batch\n",
    "        feature            = self(x)\n",
    "        embeddings         = []\n",
    "        \n",
    "        for feature in features:\n",
    "            \n",
    "            m = torch.nn.AvgPool2d(3, 1, 1)\n",
    "            embeddings.append(m(feature))\n",
    "            \n",
    "        embedding_ = embedding_concat(embeddings[0], embeddings[1])\n",
    "        self.embedding_list.extend(reshape_embedding(np.array(embedding_)))\n",
    "        \n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        \n",
    "        embedding_path = os.path.join(self.embedding_dir, 'embedding.pickle')\n",
    "        self.embedding_coreset = pickle.load(open(embedding_path, 'rb'))\n",
    "        \n",
    "        x, label, file_name, type_ = batch\n",
    "        \n",
    "        features   = self(x)\n",
    "        embeddings = []\n",
    "        \n",
    "        for feature in features:\n",
    "            \n",
    "            m = torch.nn.AvgPool2d(3, 1, 1)\n",
    "            embeddings.append(m(feature))\n",
    "            \n",
    "        embedding_     = embedding_concat(embeddings[0], embeddings[1])\n",
    "        embedding_test = np.array(reshape_embedding(np.array(embedding_)))\n",
    "        \n",
    "        knn           = KNN(torch.from_numpy(self.embedding_coreset).cuda(), k = 9)\n",
    "        score_patches = knn(torch.from_numpy(self.embedding_test).cuda())[0].cpu().detach().numpy()\n",
    "        \n",
    "        anomaly_map   = score_patches[:, 0].reshape((28, 28))\n",
    "        N_b           = score_patches[np.argmax(score_patches[:, 0])]\n",
    "        w             = (1 - (np.max(np.exp(N_b) / np.sum(np.exp(N_b)))))\n",
    "        score         = w * max(score_patches[:, 0])\n",
    "        \n",
    "        resized_map = cv2.resize(anomaly_map, (args.input_size, args.input_size))\n",
    "        blurred_map = gaussian_filter(resized_map, sigma = 4)\n",
    "        \n",
    "        self.image_path.extend(file_name)\n",
    "        self.px_lvl_pred.append(blurred_map.ravel())\n",
    "        self.img_lvl_pred.append(score)\n",
    "        \n",
    "        input_x = cv2.cvtColor(x.permute(0, 2, 3, 1).cpu().numpy[0] * 255, cv2.COLOR_BGR2RGB)\n",
    "        print(f'score : {score}')\n",
    "    \n",
    "    \n",
    "    def training_epoch_end(self, outputs):\n",
    "        \n",
    "        total_embeddings     = np.array(self.embedding_list)\n",
    "        self.randomprojector = SparseRandomProjection(n_components = 'auto', eps = 0.9)\n",
    "        self.randomprojector.fit(total_embeddings)\n",
    "        \n",
    "        selector     = KCenterGreedy(total_embeddings, 0, 0)\n",
    "        selected_idx = selector.select_batch(model = self.randomprojector,\n",
    "                                             already_selected = [], \n",
    "                                             N = int(total_embeddings.shape[0] * args.coreset_sampling_ratio))\n",
    "        \n",
    "        print(f'initial embedding size : {total_embeddings.shape}')\n",
    "        print(f'  final embedding size : {self.embedding_coreset.shape}')\n",
    "        \n",
    "        embedding_path = os.path.join(self.embedding_dir, 'embedding.pickle')\n",
    "        with open(embedding_path, 'wb') as f: pickle.dump(self.embedding_coreset, f)\n",
    "        \n",
    "        torch.save(model.state_dict(), f'{args.model_path}/model.pt')\n",
    "        \n",
    "        \n",
    "    \n",
    "    def inference(self, embedding, batch, values):\n",
    "        \n",
    "        x, file_name, folder_name = batch\n",
    "        x          = x.unsqueeze(0)\n",
    "        features   = self(x)\n",
    "        embeddings = []\n",
    "        \n",
    "        for feature in features:\n",
    "            \n",
    "            m = torch.nn.AvgPool2d(3, 1, 1)\n",
    "            embeddings.append(m(feature))\n",
    "            \n",
    "        embedding_ = embedding_concat(embeddings[0], embeddings[1])\n",
    "        embedding_test = np.array(reshape_embedding(np.array(embedding_)))\n",
    "        \n",
    "        knn           = KNN(torch.from_numpy(self.embedding_coreset).cuda(), k = 9)\n",
    "        score_patches = knn(torch.from_numpy(self.embedding_test).cuda())[0].cpu().detach().numpy()\n",
    "        \n",
    "        anomaly_map   = score_patches[:, 0].reshape((28, 28))\n",
    "        N_b           = score_patches[np.argmax(score_patches[:, 0])]\n",
    "        w             = (1 - (np.max(np.exp(N_b) / np.sum(np.exp(N_b)))))\n",
    "        score         = w * max(score_patches[:, 0])\n",
    "        \n",
    "        resized_map = cv2.resize(anomaly_map, (args.input_size, args.input_size))\n",
    "        blurred_map = gaussian_filter(resized_map, sigma = 4)\n",
    "        \n",
    "        self.image_path.extend(file_name)\n",
    "        self.px_lvl_pred.append(blurred_map.ravel())\n",
    "        self.img_lvl_pred.append(score)\n",
    "        \n",
    "        input_x = cv2.cvtColor(x.permute(0, 2, 3, 1).cpu().numpy[0] * 255, cv2.COLOR_BGR2RGB)\n",
    "        self.fixed_save_anomaly_map(blurred_map, input_x, values ,(file_name, folder_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ae05c9-9267-4fb2-9f8f-d3525b49b76a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/tensor/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:478: LightningDeprecationWarning: Setting `Trainer(gpus=1)` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=1)` instead.\n",
      "  rank_zero_deprecation(\n",
      "/opt/conda/envs/tensor/lib/python3.8/site-packages/lightning_fabric/accelerators/cuda.py:236: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습을 진행하는 기기: cuda:0\n",
      "gpu 사용 가능? :  True 사용 중인 gpu 이름 :  NVIDIA A100-SXM4-40GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/tensor/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/tensor/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=Wide_ResNet50_2_Weights.IMAGENET1K_V1`. You can also use `weights=Wide_ResNet50_2_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "/tmp/ipykernel_1325483/2824572700.py:21: DeprecationWarning: ANTIALIAS is deprecated and will be removed in Pillow 10 (2023-07-01). Use LANCZOS or Resampling.LANCZOS instead.\n",
      "  transforms.Resize((args.load_size, args.load_size), Image.ANTIALIAS),\n",
      "/opt/conda/envs/tensor/lib/python3.8/site-packages/torchvision/transforms/transforms.py:332: UserWarning: Argument 'interpolation' of type int is deprecated since 0.13 and will be removed in 0.15. Please use InterpolationMode enum.\n",
      "  warnings.warn(\n",
      "You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "/opt/conda/envs/tensor/lib/python3.8/site-packages/pytorch_lightning/core/optimizer.py:183: UserWarning: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer\n",
      "  rank_zero_warn(\n",
      "\n",
      "  | Name      | Type    | Params\n",
      "--------------------------------------\n",
      "0 | model     | ResNet  | 68.9 M\n",
      "1 | criterion | MSELoss | 0     \n",
      "--------------------------------------\n",
      "68.9 M    Trainable params\n",
      "0         Non-trainable params\n",
      "68.9 M    Total params\n",
      "275.533   Total estimated model params size (MB)\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "args   = get_args()\n",
    "\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "gpu_name = torch.cuda.get_device_name()\n",
    "device = torch.device('cuda:0' if USE_CUDA else 'cpu')\n",
    "\n",
    "print('학습을 진행하는 기기:', device)\n",
    "print('gpu 사용 가능? : ', USE_CUDA, '사용 중인 gpu 이름 : ', gpu_name)\n",
    "\n",
    "trainer = pl.Trainer.from_argparse_args(args, default_root_dir=args.output_path, \n",
    "                                        max_epochs=args.epochs, gpus=1)\n",
    "model = STPM(hparams=args)\n",
    "\n",
    "os.makedirs(args.model_path, exist_ok = True)\n",
    "if args.phase == 'train':\n",
    "  trainer.fit(model)\n",
    "  trainer.test(model)\n",
    "    \n",
    "elif args.pahse == 'test':\n",
    "  trainer.test(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a517b70c-c165-463c-9594-051c71201d4d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensor",
   "language": "python",
   "name": "tensor"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
