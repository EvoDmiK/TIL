{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "880fd86d-583a-4d7a-a74f-c717ef110e0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/tensor/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "from imutils.paths import list_images\n",
    "from collections import OrderedDict\n",
    "from torchvision import transforms\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import models\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1388f81b-9e9a-44cc-9d9f-a1a510ead627",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ROOT_PATH    = '/'.join(os.getcwd().split('/')[:-2])\n",
    "DATASET_PATH = f'{ROOT_PATH}/dataset/UTKface'\n",
    "IS_CUDA      = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "EPOCHS = 10\n",
    "LR     = 1e-3\n",
    "BS     = 64\n",
    "\n",
    "device       = torch.device(IS_CUDA)\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c32874d2-b7df-4ca8-90cc-1d4f72f3ade5",
   "metadata": {},
   "outputs": [],
   "source": [
    "digit2lb = {'gender' : {0 : 'male',  1 : 'female'},\n",
    "            'race'   : {0 : 'White', 1 : 'Black', 2 : 'Asian', 3 : 'Indian', 4 : 'Others'}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4bd0659b-29a5-4233-bd3d-e3658bfdaf86",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UTKFace(Dataset):\n",
    "    \n",
    "    def __init__(self, image_paths):\n",
    "        \n",
    "        self.transform = transforms.Compose([\n",
    "                            transforms.Resize((32, 32)),\n",
    "                            transforms.ToTensor(),\n",
    "                            transforms.Normalize([0.486, 0.456, 0.406],\n",
    "                                                [0.229, 0.224, 0.225])\n",
    "                        ])\n",
    "        \n",
    "        self.image_paths = image_paths\n",
    "        self.images      = []\n",
    "        self.genders     = []\n",
    "        self.races       = []\n",
    "        self.ages        = []\n",
    "        \n",
    "        for idx, path in enumerate(image_paths, 1):\n",
    "            filename = path.split('/')[-1].split(\"_\")\n",
    "            \n",
    "            if len(filename) == 4:\n",
    "                age, gender, race, _ = filename\n",
    "                \n",
    "                try:\n",
    "                    self.genders.append(int(gender))\n",
    "                    self.races.append(int(race))\n",
    "                    self.ages.append(int(age))\n",
    "                    self.images.append(path)\n",
    "                    \n",
    "                except:\n",
    "                    print(f'[{idx}]. {path}')\n",
    "                \n",
    "                \n",
    "    def __len__(self): return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        image = Image.open(self.images[idx]).convert('RGB')\n",
    "        image = self.transform(image)\n",
    "        \n",
    "        gender = self.genders[idx]\n",
    "        race   = self.races[idx]\n",
    "        age    = self.ages[idx]\n",
    "        \n",
    "        sample = {'image' : image, 'gender' : gender,\n",
    "                 'race'   : race,  'age'    : age}\n",
    "        \n",
    "        return sample\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8aa1cc6a-6eff-4a07-bd67-3ed479ba79e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_image_paths = sorted(list_images(f'{DATASET_PATH}/train'))\n",
    "valid_image_paths = sorted(list_images(f'{DATASET_PATH}/valid'))\n",
    "test_image_paths  = sorted(list_images(f'{DATASET_PATH}/test'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4bfb25ab-0107-49ee-a026-09f05909a949",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9391]. /home/jovyan/TIL/dataset/UTKface/valid/53__0_20170116184028385.jpg\n"
     ]
    }
   ],
   "source": [
    "train_dataloader = DataLoader(UTKFace(train_image_paths), shuffle = True,  batch_size = BS)\n",
    "valid_dataloader = DataLoader(UTKFace(valid_image_paths), shuffle = True,  batch_size = BS)\n",
    "test_dataloader  = DataLoader(UTKFace(test_image_paths),  shuffle = False, batch_size = BS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c7ccd9c7-7fe9-4e52-af9a-55b023d89412",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(-0.5, 31.5, 31.5, -0.5)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_data = next(iter(train_dataloader))\n",
    "\n",
    "gender = int(sample_data['gender'][0].data)\n",
    "image  = sample_data['image'][0]\n",
    "race   = int(sample_data['race'][0].data)\n",
    "age    = sample_data['age'][0]\n",
    "\n",
    "plt.imshow(np.transpose(image, (1, 2, 0)))\n",
    "plt.title(f'[{digit2lb[\"race\"][race]}] {digit2lb[\"gender\"][gender]} ({age} yrs old)')\n",
    "plt.axis(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fdd9f1df-03cd-454a-87c1-3f6066245cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HydraNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, net):\n",
    "        \n",
    "        super(HydraNet, self).__init__()\n",
    "        self.net        = net\n",
    "        self.n_features = self.net.fc.in_features\n",
    "        self.net.fc     = nn.Identity()\n",
    "        self.net.fc1    = nn.Sequential(\n",
    "                            OrderedDict([\n",
    "                                ('linear', nn.Linear(self.n_features, self.n_features)),\n",
    "                                ('relu1' , nn.ReLU()),\n",
    "                                ('final' , nn.Linear(self.n_features, 1))\n",
    "                            ]))\n",
    "        self.net.fc2    = nn.Sequential(\n",
    "                            OrderedDict([\n",
    "                                ('linear', nn.Linear(self.n_features, self.n_features)),\n",
    "                                ('relu1' , nn.ReLU()),\n",
    "                                ('final' , nn.Linear(self.n_features, 5))\n",
    "                            ]))\n",
    "        \n",
    "        self.net.fc3   = nn.Sequential(\n",
    "                            OrderedDict([\n",
    "                                ('linear', nn.Linear(self.n_features, self.n_features)),\n",
    "                                ('relu1' , nn.ReLU()),\n",
    "                                ('final' , nn.Linear(self.n_features, 1))\n",
    "                            ]))\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        age_head    = self.net.fc1(self.net(x))\n",
    "        race_head   = self.net.fc2(self.net(x))\n",
    "        gender_head = self.net.fc3(self.net(x))\n",
    "        \n",
    "        return age_head, race_head, gender_head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4c489ea4-98dd-456e-8b0e-e9b00d9b20f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/tensor/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/tensor/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "net   = models.resnet34(pretrained = True)\n",
    "model = HydraNet(net).to(device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "17674e3c-4afb-4351-b2bc-4cf9cdb5870b",
   "metadata": {},
   "outputs": [],
   "source": [
    "age_loss    = nn.L1Loss()\n",
    "race_loss   = nn.CrossEntropyLoss()\n",
    "gender_loss = nn.BCELoss()\n",
    "\n",
    "\n",
    "optimizer   = torch.optim.SGD(model.parameters(), LR, momentum = 0.09)\n",
    "sigmoid     = nn.Sigmoid()     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "714ba35e-637b-4d14-a3f6-53a091c1bfe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 / 10] training start\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/159 [00:00<?, ?it/s]/opt/conda/envs/tensor/lib/python3.8/site-packages/torch/nn/modules/loss.py:96: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      " 99%|█████████▉| 158/159 [01:26<00:00,  2.24it/s]/opt/conda/envs/tensor/lib/python3.8/site-packages/torch/nn/modules/loss.py:96: UserWarning: Using a target size (torch.Size([23])) that is different to the input size (torch.Size([23, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "100%|██████████| 159/159 [01:26<00:00,  1.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 / 10] train_loss : 0.477714866399765 gender acc : 44.79 race acc : 1662.03\n",
      "[2 / 10] training start\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 159/159 [01:25<00:00,  1.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 / 10] train_loss : 0.45231932401657104 gender acc : 44.79 race acc : 3259.11\n",
      "[3 / 10] training start\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 159/159 [01:24<00:00,  1.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 / 10] train_loss : 0.43253374099731445 gender acc : 44.79 race acc : 3388.76\n",
      "[4 / 10] training start\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 159/159 [01:25<00:00,  1.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4 / 10] train_loss : 0.41475045680999756 gender acc : 44.79 race acc : 3393.96\n",
      "[5 / 10] training start\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 159/159 [01:24<00:00,  1.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5 / 10] train_loss : 0.3975462317466736 gender acc : 44.79 race acc : 3396.34\n",
      "[6 / 10] training start\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 159/159 [01:24<00:00,  1.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6 / 10] train_loss : 0.3834778666496277 gender acc : 44.79 race acc : 3399.10\n",
      "[7 / 10] training start\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 159/159 [01:24<00:00,  1.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7 / 10] train_loss : 0.37242963910102844 gender acc : 44.79 race acc : 3398.70\n",
      "[8 / 10] training start\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 159/159 [01:24<00:00,  1.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8 / 10] train_loss : 0.3648986220359802 gender acc : 44.79 race acc : 3398.28\n",
      "[9 / 10] training start\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 159/159 [01:24<00:00,  1.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9 / 10] train_loss : 0.36033183336257935 gender acc : 44.79 race acc : 3395.98\n",
      "[10 / 10] training start\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 159/159 [01:25<00:00,  1.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10 / 10] train_loss : 0.3577789068222046 gender acc : 44.79 race acc : 3395.98\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, EPOCHS + 1):\n",
    "    \n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    gender_acc = 0\n",
    "    race_acc   = 0 \n",
    "\n",
    "    print(f'[{epoch} / {EPOCHS}] training start')\n",
    "    for idx, data in enumerate(tqdm(train_dataloader)):\n",
    "        \n",
    "        inputs    = data['image'].to(device = device)\n",
    "        age_lb    = data['age'].to(device = device)\n",
    "        race_lb   = data['race'].to(device = device)\n",
    "        gender_lb = data['gender'].to(device = device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        age_output, race_output, gender_output = model(inputs)\n",
    "        \n",
    "        gender_pred = gender_output.max(1, keepdim = True)[1]\n",
    "        race_pred   = race_output.max(1, keepdim = True)[1]\n",
    "        \n",
    "        gender_acc += gender_pred.eq(gender_lb.view_as(gender_pred)).sum().item()\n",
    "        race_acc   += race_pred.eq(race_lb.view_as(race_lb)).sum().item()\n",
    "        \n",
    "        loss1 = age_loss(age_output, age_lb)\n",
    "        loss2 = race_loss(race_output, race_lb)\n",
    "        loss3 = gender_loss(sigmoid(gender_output), gender_lb.unsqueeze(1).float())\n",
    "        loss  = loss1 + loss2 + loss3\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss\n",
    "    \n",
    "    train_loss /= len(train_dataloader.dataset)\n",
    "    gender_acc /= len(train_dataloader.dataset)\n",
    "    race_acc   /= len(train_dataloader.dataset)\n",
    "    \n",
    "    print(f'[{epoch} / {EPOCHS}] train_loss : {train_loss} gender acc : {gender_acc * 100:.2f} race acc : {race_acc * 100:.2f}')\n",
    "    print('\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a882ce7a-d33a-45a2-8cc8-cf7265cb4ec4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensor",
   "language": "python",
   "name": "tensor"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
