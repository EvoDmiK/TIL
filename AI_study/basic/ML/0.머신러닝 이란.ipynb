{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "546cbab0-d4f2-41f4-8686-78935992dee9",
   "metadata": {},
   "source": [
    "# **0. 참고자료**\n",
    "## **0-1. 도서**\n",
    "   - 한빛 미디어 | 오를레앙 제롱 저, 박해선 역 - 핸즈온 머신러닝\n",
    "\n",
    "## **0-2. 논문, 학술지**\n",
    "\n",
    "## **0-3. 웹사이트**\n",
    "- char :: 머신러닝 - 군집 | [[블로그 링크]](https://charstring.tistory.com/425) \n",
    "\n",
    "## **0-4. 데이터셋 출처**\n",
    "\n",
    "\n",
    "\n",
    "# **1. 머신러닝이란**\n",
    "- 명시적인 프로그래밍 없이 컴퓨터가 학습하는 능력을 갖추게 하는 연구 분야다. - 아서 새뮤얼  \n",
    "\n",
    "## **1-1. 왜 머신러닝을 사용하는가**\n",
    "- 메일의 제목을 보고 스팸 필터를 만든다고 가정하였을 때\n",
    "- 전통적인 프로그래밍 기법을 사용  \n",
    "→ 작성해야하는 규칙이 점점 길과 복잡해져 비효율적으로 동작하고, 유지보수하기 힘들어진다.  \n",
    "\n",
    "- 머신러닝 기법을 사용  \n",
    "→ 스팸 메일에서 자주 나타나는 <span style=\"color: orange\"><b>패턴을 감지</b></span>하여 어떤 단어와 구절이 스팸 메일을 판단하는 데 좋은 기준인지 자동으로 학습  \n",
    "\n",
    "<span style=\"color:orange\"><b>< ! > 머신러닝 기술을 적용하여 대용량의 데이터를 분석하여 패턴을 발견하는 것을 <span style=\"color:red\">데이터 마이닝(data mining)</span>이라 한다.</b></span>  \n",
    "    \n",
    "## **1-2. 머신러닝 시스템의 종류**  \n",
    "<a href='https://kr.mathworks.com/discovery/reinforcement-learning.html'>\n",
    "    <img src=\"https://kr.mathworks.com/discovery/reinforcement-learning/_jcr_content/mainParsys3/discoverysubsection/mainParsys/image.adapt.full.medium.png/1647932661836.png\" style = 'width: 450px;'>\n",
    "    <figcaption>[ 머신러닝 시스템의 종류 ]</figcaption>\n",
    "</a>\n",
    "\n",
    "### 1-2.1. 지도학습과 비지도 학습\n",
    "#### **(1) 지도 학습 (Supervised Learning)**  \n",
    "- 학습하고자 하는 알고리즘에 주입하는 훈련 데이터에 <span style=\"color:orange\"><b>레이블 (label)</b></span>이라는 원하는 답을 포함한다.  \n",
    "- <span style=\"color:orange\"><b>분류 (classification)</b></span>와 <span style=\"color:orange\"><b>회귀 (regression)</b></span>가 전형적인 지도학습이다.  \n",
    "    \n",
    "<span style=\"color: orange\"><b>< ! > 머신러닝에서 <span style=\"color: red\">속성 (attribute)</span>은 데이터 타입을 의미한다.</b></span>\n",
    "<span style=\"color: orange\"><b></b></span>  \n",
    "<span style=\"color: orange\"><b>< ! > 머신러닝에서 <span style=\"color: red\">특성 (feature)</span>는 일반적으로 속성과 값이 합쳐진 것을 의미한다.</b></span>\n",
    "<span style=\"color: orange\"><b></b></span>  \n",
    "    \n",
    "- **분류 (Classification)**  \n",
    "→ 주어진 데이터를 클래스 별로 구별해가는 과정   \n",
    "\n",
    "- **회귀 (Regression)**  \n",
    "    → 특성을 사용하여 <span style=\"color:orange\"><b>타깃 (Target)</b></span> 수치를 예측하는 과정  \n",
    "    \n",
    "- 일부 회귀 알고리즘은 분류에 사용할 수 있고, 반대로 일부 분류 알고리즘을 회귀에 사용할 수 있다.  \n",
    "#### **(1)-1. 지도학습 알고리즘  예시**  \n",
    "    1) k-최근접 이웃 (k-Nearest Neighbors | kNN)  \n",
    "    2) 선형 회귀 (linear regression)  \n",
    "    3) 로지스틱 회귀 (logistic regression)  \n",
    "    4) 서포트 벡터 머신 (Support Vector Machine | SVM)  \n",
    "    5) 결정 트리 (Decision tree)  \n",
    "    6) 랜덤 포레스트 (random forest)  \n",
    "    7) 신경망 (Neural Network | NN)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d95492-4d2e-4f88-b146-529eef0c1e19",
   "metadata": {},
   "source": [
    "#### **(2) 비지도 학습 (Unsupervised Learning)**  \n",
    "- 지도 학습과 달리 훈련 데이터에 레이블이 없이 학습해야 함.\n",
    "\n",
    "#### **(2)-1. 비지도학습 알고리즘  예시**  \n",
    "    1)  군집 (clustering)  \n",
    "        1)-1. k-평균 (k-means)   \n",
    "        1)-2. DBSCAN  \n",
    "        1)-3. 계층 군집 분석(Hierarchical Cluseter Analysis | HCA)  \n",
    "        1)-4. 이상치 탐지(outlier detection)와 특이치 탐지(novelty detection)    \n",
    "        1)-5. 원-클래스 SVM (one-class SVM)  \n",
    "        1)-6. 아이솔레이션 포레스트 (isolation forest)  \n",
    "        \n",
    "    2) 시각화 (visualization)와 차원 축소 (dimensionality reduction)\n",
    "        2)-1. 주성분 분석 (Principal Component Analysis | PCA)  \n",
    "        2)-2. 커널 PCA (kernel PCA)  \n",
    "        2)-3. 지역적 선형 임베딩 (Locally Linear Embedding)  \n",
    "        2)-4. t-SNE (t-distributed Stochastic Neighbor Embedding)  \n",
    "        \n",
    "    3) 연관 규칙 학습 (association rule learning)  \n",
    "        3)-1. 어프라이어리 (Apriori)  \n",
    "        3)-2. 이클렛 (Eclat)  \n",
    "\n",
    "- **군집 (Clustering)**  \n",
    "→ 대상을 군집 (Clustering)이라 불리는 상대적으로 동질적인 집단으로 분류하는 기법  \n",
    "→ <span style=\"color:orange\"><b>계층 군집 (Hierarchical Clustering)</b></span>알고리즘을 사용하면 각 그룹을 더 작은 그룹으로 세분화 할 수 있다.\n",
    "<a href = 'https://velog.io/@jhlee508/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-K-%ED%8F%89%EA%B7%A0K-Means-%EC%95%8C%EA%B3%A0%EB%A6%AC%EC%A6%98'>\n",
    "    <img src=\"https://velog.velcdn.com/images%2Fjhlee508%2Fpost%2F1f8e36c9-a051-42d2-a97c-9c20211f9236%2Fimage.png\" style=\"width: 450px;\">\n",
    "    <figcaption>[ K-Means 군집화 알고리즘의 예시 ]</figcaption>\n",
    "</a>\n",
    "\n",
    "- **시각화 (Visualization)**  \n",
    "→ 레이블이 없는 대규모의 고차원 데이터를 넣으면 도식화가 가능한 2D나 3D 표현을 만들어줌.  \n",
    "→ 알고리즘은 가능한 한 구조를 그대로 유지하려 하므로 데이터가 어떻게 조작되어 있는지 이해할 수 있다.  \n",
    "→ 비슷한 작업으로는 많은 정보를 잃지 않으며 데이터를 간소화하는 <span style=\"color: orange\"><b>차원 축소</b></span>가 있다.  \n",
    "\n",
    "<span style=\"color: orange\"><b>< ! > 차원축소 기법 중 하나는 상관관계가 있는 여러 특성을 하나로 합치는 것이 있는데, 이를 <span style=\"color: red\">특성 추출 (Feature Extraction)</span>이라 한다.</b></span>  \n",
    "<span style=\"color: orange\"><b>< ! >지도 학습 같은 머신러닝 알고리즘에 데이터를 주입하기 전에 차원 축소 알고리즘을 사용하여 데이터의 차원을 줄이는 것이 유용할 때가 많다.  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;→ 실행 속도가 빨라지고, 디스크와 메모리를 차지아는 공간도 줄고 성능이 향상될 수도 있다.\n",
    "</b></span>  \n",
    "<a href='https://gaussian37.github.io/ml-concept-t_sne/'>\n",
    "    <img src=\"https://gaussian37.github.io/assets/img/ml/concept/t-sne/0.png\" style = 'width: 450px;'>\n",
    "    <figcaption>[ MNIST 데이터 셋을 이용한 t-SNE 예제 ]</figcaption>\n",
    "</a>\n",
    "    \n",
    "- **이상치 탐지(Outlier Detection | Anomaly Detection)**  \n",
    "→ 훈련하는 동안 대부분 정상 샘플을 만나 이상치를 인식하도록 훈련됨.  \n",
    "→ 새로운 샘플을 보고 정상 데이터인지 이상치인지 판단함.  \n",
    "→ 비슷한 작업으로 <span style=\"color: orange\"><b>특이치 탐지</b></span>가 있다.  \n",
    "→ 특이치 탐지는 훈련 세트에 있는 모든 샘플과 달라 보이는 새로운 샘플을 탐지하는 것이 목적이다. \n",
    "<a href='https://tensorflow.blog/%ED%95%B8%EC%A6%88%EC%98%A8-%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-1%EC%9E%A5-2%EC%9E%A5/1-3-%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-%EC%8B%9C%EC%8A%A4%ED%85%9C%EC%9D%98-%EC%A2%85%EB%A5%98/'>\n",
    "    <img src=\"https://tensorflowkorea.files.wordpress.com/2018/05/e18489e185b3e1848fe185b3e18485e185b5e186abe18489e185a3e186ba-2018-05-24-e1848be185a9e1848ce185a5e186ab-12-16-02.png?w=625\" style=\"widh: 450px;\">\n",
    "    <figcaption>[ 이상치탐지 예시 ]</figcaption>\n",
    "</a>\n",
    "\n",
    "- **연관 규칙 학습 (Association rule learning)**  \n",
    "→ 대량의 데이터에서 특성 간 관계를 찾는 작업  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a54098f-f9f5-41c0-ba3b-b54468230efa",
   "metadata": {},
   "source": [
    "#### **(3) 준지도 학습 (Semi-supervised Learning)**  \n",
    "- 데이터에 레이블을 달기에 많은 시간과 비용이 소모되어 일반적으로 레이블이 없는 샘플이 많고 레이블된 샘플이 적다.  \n",
    "- 준지도 학습에서는 일부만 레이블이 있는 데이터를 학습시키는데 사용된다.  \n",
    "- 대부분의 준지도 학습 알고리즘은 지도 학습과 비지도 학습의 조합으로 이루어져 있다.  \n",
    "- 준지도 학습의 일종인 <span style=\"color:orange\"><b>심층 신뢰 신경망 (Deep Belief Network | DBN)</b></span>은 여러 겺으로 쌓은 <span style=\"color:orange\"><b>제한된 볼츠만 머신(Restricted Boltzmann Machine)</b></span>이라 불리는 비지도 학습에 기초한다.  \n",
    "\n",
    "#### **(4) 강화 학습 (Reinforcement Learning)**  \n",
    "<a href='http://www.aitimes.com/news/articleView.html?idxno=136181'>\n",
    "    <img src=\"https://cdn.aitimes.com/news/photo/202102/136181_135726_337.jpg\" style = 'width: 450px;'>\n",
    "    <figcaption>[ 강화학습 동작의 쉬운 예 ]</figcaption>\n",
    "</a>\n",
    "\n",
    "- 알고리즘이 학습 하는 시스템을 <span style=\"color:orange\"><b>에이전트 (Agent)</b></span>라 부른다.\n",
    "- <span style=\"color:orange\"><b>환경 (Environment)</b></span>을 관찰하여 <span style=\"color:orange\"><b>행동 (Action)</b></span>을 실행하고 그 결과로 <span style=\"color:orange\"><b>보상 (Reward)</b></span> 혹은 <span style=\"color:orange\"><b>벌점 (Panalty)</b></span>를 받는다.  \n",
    "- 시간이 지나면서 가장 큰 보상을 얻기 위해 <span style=\"color:orange\"><b>정책 (Policy)</b></span>라 부르는 최상의 전략을 스스로 학습\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf8bde6e-39dd-4f51-8304-076ed60e0be5",
   "metadata": {},
   "source": [
    "## **1-3. 배치 학습과 온라인 학습**\n",
    "### 1-3.1. 배치 학습 (Batch Learning)  \n",
    "→ 가용한 데이터를 모두 사용해 훈련시켜야 한다.  \n",
    "→ 일반적으로 이 방식은 시간과 자원을 많이 소모하여 보통 오프라인에서 수행된다.  \n",
    "\n",
    "### 1-3.2. 온라인 학습 (Online Learning)  \n",
    "→ 데이터를 순차적으로 한 개씩 또는 <span style=\"color: orange\"><b>미니 배치(mini batch)</b></span>라 부르는 작은 묶음 단위로 주입하여 시스템을 훈련시킴.  \n",
    "→ 매 학습 단계가 빠르고 비용이 적게 들어 시스템은 데이터가 도착하는 대로 즉시 학습할 수 있다.  \n",
    "→ 컴퓨터 한 대의 메모리에 들어갈 수 없는 아주 큰 데이터셋을 학습하는 시스템에도 온라인 학습 알고리즘을 사용할 수 있다. (외부 메모리 학습)  \n",
    "<span style=\"color: orange\"><b>< ! > 외부 메모리 학습은 보통 오프라인으로 실행되어 온라인 학습보다는 <span style=\"color: red\"><b>점진적 학습 (incremental learning)</b></span>이라 한다.</b></span>  \n",
    "    \n",
    "→ 변화하는 데이터에 얼마나 빠르게 적응하는지를 나타내는 파라미터를 <span style=\"color:orange\"><b>학습률 (learning rate)</b></span>라고 한다.  \n",
    "(1) 학습률을 높게하면 시스템이 데이터에 <span style=\"color:orange\"><b>빠르게 적응</b></span>하지만 예전 데이터를 <span style=\"color:orange\"><b>금방 잊는다.</b></span>  \n",
    "(2) 학습률이 낮으면 <span style=\"color:orange\"><b>더 느리게 학습되지만</b></span>, 새로운 데이터에 있는 잡음이나 대표성 없는 <span style=\"color:orange\"><b>데이터 포인트에 덜 민감해진다.</b></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eabd5cae-aeb3-4af3-84b2-99a868d3e5ab",
   "metadata": {
    "tags": []
   },
   "source": [
    "## **1-4. 사례 기반 학습과 모델 기반 학습**\n",
    "→ 주어진 훈련 데이터로 학습하고 훈련 데이터에서 본 적 없는 새로운 데이터에서 좋은 예측을 만들어야<span style=\"color:orange\"><b>(일반화 / generalize)</b></span>한다.  \n",
    "→ 일반화를 위한 두 가지 접근법은 <span style=\"color:orange\"><b>사례 기반 학습</b></span>과 <span style=\"color:orange\"><b>모델 기반 학습</b></span>이 있다.  \n",
    "### 1-4.1. 사례기반 학습(instance-based learning)  \n",
    "- 시스템이 훈련 샘픙을 기억함으로써 학습 시킴.  \n",
    "- 유사도 특정을 사용하여 새로운 데이터와 학습한 샘플을 비교하는 식으로 일반화 함.  \n",
    "\n",
    "### 1-4.2. 모델기반 학습(model-based learning)  \n",
    "- 샘플들의 모델을 만들어 <span style=\"color:orange\"><b>예측(prediction)</b></span>에 사용하는 것  \n",
    "\n",
    "## **1-5. 머신러닝 주요 도전과제**\n",
    "- 머신러닝 알고리즘 학습에 문제가 될만한 요소로는 <span style=\"color: orange\"><b>나쁜 데이터</b></span>와 <span style=\"color: orange\"><b>나쁜 알고리즘</b></span>이 있다.  \n",
    "\n",
    "### 1-5.1. 나쁜 데이터\n",
    "#### **(1) 충분하지 않은 양의 훈련 데이터**\n",
    "- 대부분의 머신러닝 알고리즘이 잘 작동하려면 데이터가 많아야 한다.  \n",
    "\n",
    "#### **(2) 대표성 없는 훈련 데이터**\n",
    "- 일반화하려는 사례들을 대표하는 훈련 세트를 사용하는 것이 매우 중요하지만, 어려운 경우가 많다.  \n",
    "- 샘플이 작으면 <span style=\"color: orange\"><b>샘플링 잡음 (Sampling noise)</b></span>이 생기고,  \n",
    "  샘플이 매우 큰 경우엔 표본 추출방법이 잘못되면 대표성을 띠지 못하는 <span style=\"color: orange\"><b>샘플링 편향 (Sampling bias)</b></span>이 발생한다. \n",
    "\n",
    "#### **(3) 낮은 품질의 데이터**\n",
    "- 훈련 데이터가 에러, 이상치 (Anomaly / Outlier), 잡음으로 가득하다면 데이터에 존재하는 패턴을 찾기 어려워 잘 작동하지 않는다.  \n",
    "- 이런 경우에 훈련 데이터를 정제하는 작업이 필요하다.  \n",
    "    1) 일부 샘플이 이상치라는게 명확하면 간단히 그것들을 무시하거나 수동으로 잘못된 것을 고치는 것이 좋다.  \n",
    "    2) 일부 샘플에 <span style=\"color: orange\"><b>결측치</b></span>가 존재한다면, 결측치 데이터를 무시할지 빠진 값을 채울지 결정해야한다.\n",
    "    \n",
    "#### **(4) 관련없는 특성**\n",
    "- <span style=\"color: orange\"><b>garbage in garbage out</b></span>이라는 말이 있듯이 훈련 데이터에 관련없는 특성이 적고 관련 있는 특성이 충분해야 학습할 수 있다. \n",
    "- 머신러닝 프로젝트의 핵심요소는 훈련에 사용할 좋은 특성들을 찾는 것이다.  \n",
    "- 이 과정을 <span style=\"color: orange\"><b>특성 공학 (feature engineering)</b></span>이라 하며 다음과 같은 작업을 한다.  \n",
    "    1) <span style=\"color: orange\"><b>특성 선택 (feature selection)</b></span>  | 가지고 있는 특성 중 훈련에 가장 유용한 특성을 선택한다.  \n",
    "    2) <span style=\"color: orange\"><b>특성 추출 (feature extraction)</b></span> | 특성을 결합하여 더 유용한 특성을 만든다.  \n",
    "    3) 새로운 데이터를 수집해 새 특성을 만든다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "660118fc-9ac3-4112-afcd-bd19159a5010",
   "metadata": {},
   "source": [
    "### 1-5.2. 나쁜 알고리즘\n",
    "<a href='https://wikidocs.net/152777'>\n",
    "    <img src=\"https://wikidocs.net/images/page/152777/overfit.JPG\" style = 'width: 450px;'>\n",
    "    <figcaption>[ 과소적합, 과대적합 ]</figcaption>\n",
    "</a>\n",
    "\n",
    "#### **(1) 훈련 데이터 과대적합**\n",
    "- 모델이 훈련 데이터 셋에 정확도가 높게 나오지만, 시험 / 검증 데이터 셋에 대해서는 정확도가 낮게 나오는 현상을 <span style=\"color: orange\"><b>과대적합 (Overfitting)</b></span>이라 한다.  \n",
    "- 훈련 데이터에 있는 잡읍의 양에 비해 모델이 너무 복잡할 때 일어나며 해결 방법은 다음과 같다.  \n",
    "  1) 파라미터 수가 적은 모델을 선택하거나, 훈련 데이터에 있는 특성 수를 줄이거나, 모델에 제약을 가해 단순화 시킨다.  \n",
    "  2) 훈련 데이터의 잡음을 줄인다. (오류 데이터 수정과 이상치 제거)\n",
    "  3) 훈련 데이터를 더 모은다.\n",
    "- 과대적합의 위험을 감소시키기 위해 모델에 제약을 가하는 것을 <span style=\"color: orange\"><b>규제 (Regularization)</b></span>라고 한다.  \n",
    "- 학습하는 동안 적용할 규제의 양은 <span style=\"color: orange\"><b>하이퍼파라미터 (Hyperparameter)</b></span>가 결정한다.  \n",
    "<span style=\"color: red\"><b>! 하이퍼 파라미터는 모델이 아니라 학습 알고리즘의 파라미터로, 훈련전에 미리 지정되고, 훈련하는 동안에 상수로 남아있다.</b></span>\n",
    "\n",
    "<a href='https://in.pinterest.com/pin/368873025699369310/'>\n",
    "    <img src=\"https://i.pinimg.com/originals/b0/f3/1a/b0f31a541130011123c67117d0e461a3.png\" style = 'width: 450px;'>\n",
    "    <figcaption>[ 오버피팅의 쉬운 예 ]</figcaption>\n",
    "</a>\n",
    "\n",
    "#### **(2) 훈련 데이터 과소적합**\n",
    "- 과대적합의 반대 개념으로, 모델이 너무 단순해 데이터의 내재된 구조를 학습하지 못하는 것을 <span style=\"color: orange\"><b>과소적합 (underfitting)</b></span>이라고 한다.  \n",
    "- 현실은 모델보다 더 복잡하므로 훈련 샘플에서도 부정확한 예측을 만드며 해결 방법은 다음과 같다.\n",
    "   1) 모델 파라미터가 더 많은 모델을 선택한다.\n",
    "   2) 학습 알고리즘에 더 좋은 특성을 제공한다. \n",
    "   3) 모델의 제약을 줄인다.\n",
    "   \n",
    "<a href='https://www.businessinsider.com/the-funniest-homework-assignment-answers-2015-11'>\n",
    "    <img src=\"https://i.insider.com/5651cda7dd08950b058b45e5?width=750&format=jpeg&auto=webp\" style = 'width: 450px;'>\n",
    "    <figcaption>[ 언더피팅의 쉬운 예 ]</figcaption>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d1f876-0e7a-4152-a02d-04cbcf785c0d",
   "metadata": {},
   "source": [
    "## **1-6. 테스트와 검증**\n",
    "- 모델이 새로운 샘플에 얼마나 잘 일반화 되는지 알아보는 방법은 새로운 데이터를 모델에 적용해 보는 것이다.  \n",
    "- 데이터 셋을 <span style=\"color: orange\"><b>훈련용</b></span>과 <span style=\"color: orange\"><b>시험용</b></span>으로 나누어 테스트한다.\n",
    "- 새로운 샘플에 대한 오류 비율을 <span style=\"color: orange\"><b>일반화 오차(generalization error / 외부 샘플 오차 out of sample error)</b></span>라고 하며,  \n",
    "  시험용 데이터 셋으로 모델을 평가하여 오차에 대한 <span style=\"color: orange\"><b>추정값 (estimation)</b></span>을 얻는다.  \n",
    "  \n",
    "<a href='https://jjeongil.tistory.com/922'>\n",
    "    <img src=\"https://blog.kakaocdn.net/dn/b0Um8Y/btqAMOZsELS/PKP6guQbIjEZe4OWE9DUT0/img.png\" style = 'width: 450px;'>\n",
    "    <figcaption>[ 데이터 셋 분할 ]</figcaption>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f4be40-f063-4887-8694-c695eae42838",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bio",
   "language": "python",
   "name": "bio"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
